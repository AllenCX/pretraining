{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pretrain as pt\n",
    "from datasets import load_dataset\n",
    "from torch.nn import functional as F\n",
    "from transformers import PreTrainedModel, AutoTokenizer ,AutoModelForCausalLM\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_ppl(\n",
    "    text, \n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    stride: int = 512,\n",
    "    device=None,\n",
    "):\n",
    "\n",
    "    if device is not None:\n",
    "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "        if device == \"gpu\":\n",
    "            device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    # if batch_size > 1 (which generally leads to padding being required), and\n",
    "    # if there is not an already assigned pad_token, assign an existing\n",
    "    # special token to also be the padding token\n",
    "    if tokenizer.pad_token is None and stride > 1:\n",
    "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "        # check that the model already has at least one special token defined\n",
    "        assert (\n",
    "            len(existing_special_tokens) > 0\n",
    "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "        # assign one of the special tokens to also be the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "    # if add_start_token and max_length:\n",
    "    #     # leave room for <BOS> token to be added:\n",
    "    #     assert (\n",
    "    #         tokenizer.bos_token is not None\n",
    "    #     ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "    #     max_tokenized_len = max_length - 1\n",
    "    # else:\n",
    "    #     max_tokenized_len = max_length\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        # add_special_tokens=False,\n",
    "        # padding=True,\n",
    "        truncation=False,        \n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    ).to(device)    \n",
    "    \n",
    "    print(encodings)\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    losses = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        # Computes how much overlap there is with the previous batch.\n",
    "        new_tokens_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        attn_mask = encodings.attention_mask[:, begin_loc:end_loc].to(device)\n",
    "        labels = input_ids.clone()\n",
    "        # Ignore the tokens we've processed on a previous batch. -100 is a magic\n",
    "        # value that is ignored by the CrossEntropyLoss function\n",
    "        labels[:, :-new_tokens_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(input_ids, attention_mask=attn_mask).logits\n",
    "\n",
    "        # Shift by 1 token.\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "        # Flatten the tensors\n",
    "        shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        shift_attention_mask = shift_attention_mask.view(-1)\n",
    "        print((loss_fct(shift_logits, shift_labels) * shift_attention_mask))\n",
    "        losses.append((loss_fct(shift_logits, shift_labels, ignore_index=-100) * shift_attention_mask).sum(0))\n",
    "    return torch.exp(losses.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(\"cpu\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "text = \"\\n\\n\".join(dataset[\"text\"])\n",
    "\n",
    "print(compute_ppl(text, model, tokenizer, 1024, \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/dwoods/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dwoods/Programming/pretraining/scripts/wandb/run-20240210_121947-0700fgty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raofoundation/pretraining-leaderboard-data/runs/0700fgty' target=\"_blank\">bright-ox-2</a></strong> to <a href='https://wandb.ai/raofoundation/pretraining-leaderboard-data' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raofoundation/pretraining-leaderboard-data' target=\"_blank\">https://wandb.ai/raofoundation/pretraining-leaderboard-data</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raofoundation/pretraining-leaderboard-data/runs/0700fgty' target=\"_blank\">https://wandb.ai/raofoundation/pretraining-leaderboard-data/runs/0700fgty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-ox-2</strong> at: <a href='https://wandb.ai/raofoundation/pretraining-leaderboard-data/runs/0700fgty' target=\"_blank\">https://wandb.ai/raofoundation/pretraining-leaderboard-data/runs/0700fgty</a><br/>Synced 4 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240210_121947-0700fgty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume the perplexity calculation is correct...\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "\n",
    "PROJECT=\"pretraining-leaderboard-data\"\n",
    "ENTITY=\"raofoundation\"\n",
    "id = \"test-run\"\n",
    "token = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Continue the wandb run\n",
    "wandb.login(key=token)\n",
    "run = wandb.init(project=PROJECT, entity=ENTITY)\n",
    "\n",
    "datasets = {}\n",
    "models = {}\n",
    "\n",
    "ppls = defaultdict(list)\n",
    "for name, model in models:\n",
    "    for name, dataset in datasets.items():\n",
    "        ppls[name].append(compute_ppl(dataset, model, tokenizer, 1024, \"cpu\"))\n",
    "        \n",
    "# 1 run per \"run\". Will then have to query the history to get old data.\n",
    "table = wandb.Table(dataframe=pd.DataFrame(ppls, index=models.keys()))\n",
    "wandb.log({\"benchmarks\": table})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_benchmarks(entity, project) -> pd.DataFrame:\n",
    "    api = wandb.Api(timeout=100)\n",
    "    # By default, runs are sorted in descending order by creation time.\n",
    "    runs = list(api.runs(\n",
    "        f\"{entity}/{project}\",\n",
    "    ))\n",
    "\n",
    "    for run in runs:\n",
    "        artifacts = list(run.logged_artifacts())\n",
    "        if artifacts:\n",
    "            table = artifacts[-1].get(\"benchmarks\")\n",
    "            if table:\n",
    "                return table.get_dataframe()\n",
    "    raise ValueError(\"No benchmarks found\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          wikitext    falcon\n",
      "chatgpt2         1     0.234\n",
      "my-model         2  2342.000\n"
     ]
    }
   ],
   "source": [
    "models={\"chatgpt2\": None, \"my-model\": None}\n",
    "ppls = {\"wikitext\": [1, 2], \"falcon\": [0.234, 2342]}\n",
    "\n",
    "df = pd.DataFrame(ppls, index=models.keys())\n",
    "print(df)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
